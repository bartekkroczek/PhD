





# Synthetic sample data: Replace with your actual data
np.random.seed(0)  # For reproducibility
time_series = np.random.randn(150, 100)  # 150 participants, 100 time points each


# Example using Discrete Wavelet Transform on the first participant's data
participant = time_series[0, :]  # First participant's data

# Perform DWT
coeffs = pywt.wavedec(participant, 'db1', level=None)  # 'db1' denotes Daubechies wavelet with 1 vanishing moment

# Plot the coefficients at each level
plt.figure(figsize=(12, 8))
for i, coeff in enumerate(coeffs):
    plt.subplot(len(coeffs), 1, i+1)
    plt.plot(coeff)
    plt.title(f'DWT Level {i}')
plt.tight_layout()
plt.show()


variances = [np.var(coeff) for coeff in coeffs]

plt.figure(figsize=(8, 4))
plt.plot(variances, marker='o')
plt.xlabel('Decomposition Level')
plt.ylabel('Variance of Wavelet Coefficients')
plt.title('Variance Across Decomposition Levels')
plt.xticks(range(len(variances)))
plt.grid(True)
plt.show()


# Load the data
data_path = '../aggr_data.csv'
data = pd.read_csv(data_path)

# Initialize a dictionary to hold variance data
variances_across_participants = {}

# Define the wavelet to use
wavelet = 'db4'

# Loop over each participant
for part_id, group in data.groupby('PART_ID'):
    time_series = group['mean_corr'].values
    coeffs = pywt.wavedec(time_series, wavelet, mode='per', level = None)
    variances = [np.var(coeff) for coeff in coeffs]
    variances_across_participants[part_id] = variances


pywt.dwt_max_level()


# Convert the variances to a DataFrame for easier analysis
variance_df = pd.DataFrame(variances_across_participants).T

# Calculate the mean variance across participants for each decomposition level
mean_variances = variance_df.mean()

# Plot the mean variance across decomposition levels
plt.figure(figsize=(10, 6))
plt.plot(mean_variances, marker='o', linestyle='-')
plt.xlabel('Decomposition Level')
plt.ylabel('Mean Variance of Wavelet Coefficients')
plt.title('Mean Variance Across Participants at Each Decomposition Level')
plt.grid(True)
plt.show()

# This plot can help identify if there's a consistent level where variance (and thus potential periodicity) is more pronounced.


import numpy as np
import pandas as pd
from scipy.stats import ttest_1samp

# Assuming variance_df is your DataFrame of variances across participants with rows as participants and columns as decomposition levels

def permutation_test(data, n_permutations=1000):
    """
    Performs a permutation test on the data to assess the significance of the observed mean variance.
    
    Parameters:
    - data: DataFrame containing the variances of wavelet coefficients across participants
    - n_permutations: Number of permutations to perform
    
    Returns:
    - p_values: P-values for each decomposition level, assessing the significance of the observed mean variance
    """
    observed_means = data.mean()
    p_values = pd.Series(index=observed_means.index, dtype=float)
    
    for level in data.columns:
        observed_mean = observed_means[level]
        distribution = []
        
        for _ in range(n_permutations):
            # Shuffle the data within each level
            shuffled = data[level].sample(frac=1, replace=False).reset_index(drop=True)
            distribution.append(shuffled.mean())
        
        # Calculate the p-value as the proportion of shuffled means that are greater than or equal to the observed mean
        p_value = (np.sum(distribution >= observed_mean) + 1) / (n_permutations + 1)
        p_values[level] = p_value
    
    return p_values

# Perform the permutation test
p_values = permutation_test(variance_df, n_permutations=1000)

# Display the p-values
print(p_values)


import numpy as np
from matplotlib import pyplot as plt
import pycwt as wavelet
from pycwt.helpers import find

# Because the time series might not be uniformly sampled, we create a uniform time series for the wavelet analysis.
# Here, we assume 't' is approximately uniformly spaced for simplification. For more precise analysis, interpolation might be needed.

# Aggregate data over time if necessary
t = data['t'].unique()  # Assuming 't' values are uniformly spaced and consistent across participants
mean_corr = data.groupby('t')['mean_corr'].mean().values  # Average mean_corr at each time point across all participants

# Wavelet Transform
dt = np.mean(np.diff(t))  # Calculate the mean time difference
n = len(t)
time = np.arange(0, n) * dt  # Create an array of time points
p = np.polyfit(time, mean_corr, 1)  # Remove the linear trend for a better wavelet analysis
mean_corr_detrend = mean_corr - np.polyval(p, time)

# Define the wavelet to be used
mother = wavelet.Morlet(6)
s0 = 2 * dt  # Starting scale
dj = 1 / 12  # Twelve sub-octaves per octaves
J = 7 / dj  # Seven powers of two with dj sub-octaves
alpha, _, _ = wavelet.ar1(mean_corr_detrend)  # Lag-1 autocorrelation for white noise

# The following wavelet transform returns the wavelet power spectrum (WPS), scales, frequencies, coi, fft, fft frequencies, and significance levels.
WPS, scales, freqs, coi, fft, fftfreqs, sig = wavelet.cwt(mean_corr_detrend, dt, dj, s0, J, mother)
power = (np.abs(WPS)) ** 2
significance_levels = wavelet.significance(1.0, dt, scales, 0, alpha, significance_level=0.95, wavelet=mother)

# Visualizing the wavelet power spectrum
plt.figure(figsize=(10, 6))
plt.contourf(time, np.log2(freqs), np.log2(power), extend='both')
plt.colorbar(label='Power (log2 scale)')
plt.title('Wavelet Power Spectrum of mean_corr')
plt.ylabel('Frequency (log2 scale)')
plt.xlabel('Time')
plt.tight_layout()

# Overlay the cone of influence
plt.contour(time, np.log2(freqs), sig, [-99, 1], colors='k', linewidths=2)
plt.fill(np.concatenate([time, time[-1:] + dt, time[-1:] + dt, time[:1] - dt, time[:1] - dt]),
         np.concatenate([np.log2(coi), [1e-9], np.log2(freqs)[-1:], np.log2(freqs)[-1:], [1e-9]]),
         'k', alpha=0.3, hatch='x')

# Indicate the significance levels with a contour
plt.contour(time, np.log2(freqs), np.log2(power) - np.log2(significance_levels[:, None]), [-99, 0], colors='r', linewidths=2)

plt.show()

# Calculate the global wavelet spectrum and its significance level
global_ws = power.mean(axis=1)
dof = n - scales  # Degrees of freedom
global_significance = wavelet.significance(1.0, dt, scales, 1, alpha, significance_level=0.95, dof=dof, wavelet=mother)

# Plot the global wavelet spectrum
plt.figure(figsize=(8, 4))
plt.plot(np.log2(freqs), np.log2(global_ws), 'b-', label='Global Wavelet Spectrum')
plt.plot(np.log2(freqs), np.log2(global_significance), 'r--', label='Significance Level')
plt.xlabel('Frequency (log2 scale)')
plt.ylabel('Power (log2 scale)')
plt.title('Global Wavelet Spectrum')
plt.legend()
plt.tight_layout()
plt.show()

# Assessing the presence of significant periodicity
significant_periods = freqs[np.where(global_ws >= global_significance[0])]
p_value = 1 - 0.95 if significant_periods.size > 0 else 1  # Simplistic approach to demonstrate significance

significant_periods, p_value

