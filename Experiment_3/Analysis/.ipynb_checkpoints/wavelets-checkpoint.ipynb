{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"wavelets\"\n",
        "author: \"Bartek Kroczek\"\n",
        "format: html\n",
        "editor: source\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ```{python}\n",
        "import yaml\n",
        "import pywt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the data\n",
        "data_path = '../aggr_data.csv'\n",
        "data = pd.read_csv(data_path)\n",
        "\n",
        "# Display the first few rows of the dataframe to understand its structure\n",
        "data.head()\n",
        "\n",
        "# Since the focus is on demonstrating the process, we'll select one participant as an example.\n",
        "example_participant_id = data['PART_ID'].unique()[0]\n",
        "example_data = data[data['PART_ID'] == example_participant_id]['mean_corr'].values\n",
        "\n",
        "# DWT: Discrete Wavelet Transform\n",
        "wavelet = 'db4'  # Daubechies wavelet with 4 vanishing moments, chosen for its good balance between time and frequency localization.\n",
        "levels = pywt.dwt_max_level(data_len=len(example_data), filter_len=pywt.Wavelet(wavelet).dec_len)\n",
        "\n",
        "# Perform the decomposition\n",
        "coeffs = pywt.wavedec(data=example_data, wavelet=wavelet, level=levels)\n",
        "\n",
        "# Plot the coefficients\n",
        "plt.figure(figsize=(10, 6))\n",
        "for i, coeff in enumerate(coeffs):\n",
        "    plt.subplot(len(coeffs), 1, i+1)\n",
        "    plt.plot(coeff)\n",
        "    plt.title(f'Level {i} DWT Coefficients')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Synthetic sample data: Replace with your actual data\n",
        "np.random.seed(0)  # For reproducibility\n",
        "time_series = np.random.randn(150, 100)  # 150 participants, 100 time points each"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Example using Discrete Wavelet Transform on the first participant's data\n",
        "participant = time_series[0, :]  # First participant's data\n",
        "\n",
        "# Perform DWT\n",
        "coeffs = pywt.wavedec(participant, 'db1', level=None)  # 'db1' denotes Daubechies wavelet with 1 vanishing moment\n",
        "\n",
        "# Plot the coefficients at each level\n",
        "plt.figure(figsize=(12, 8))\n",
        "for i, coeff in enumerate(coeffs):\n",
        "    plt.subplot(len(coeffs), 1, i+1)\n",
        "    plt.plot(coeff)\n",
        "    plt.title(f'DWT Level {i}')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "variances = [np.var(coeff) for coeff in coeffs]\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(variances, marker='o')\n",
        "plt.xlabel('Decomposition Level')\n",
        "plt.ylabel('Variance of Wavelet Coefficients')\n",
        "plt.title('Variance Across Decomposition Levels')\n",
        "plt.xticks(range(len(variances)))\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load the data\n",
        "data_path = '../aggr_data.csv'\n",
        "data = pd.read_csv(data_path)\n",
        "\n",
        "# Initialize a dictionary to hold variance data\n",
        "variances_across_participants = {}\n",
        "\n",
        "# Define the wavelet to use\n",
        "wavelet = 'db4'\n",
        "\n",
        "# Loop over each participant\n",
        "for part_id, group in data.groupby('PART_ID'):\n",
        "    time_series = group['mean_corr'].values\n",
        "    coeffs = pywt.wavedec(time_series, wavelet, mode='per', level = None)\n",
        "    variances = [np.var(coeff) for coeff in coeffs]\n",
        "    variances_across_participants[part_id] = variances"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pywt.dwt_max_level()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Convert the variances to a DataFrame for easier analysis\n",
        "variance_df = pd.DataFrame(variances_across_participants).T\n",
        "\n",
        "# Calculate the mean variance across participants for each decomposition level\n",
        "mean_variances = variance_df.mean()\n",
        "\n",
        "# Plot the mean variance across decomposition levels\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(mean_variances, marker='o', linestyle='-')\n",
        "plt.xlabel('Decomposition Level')\n",
        "plt.ylabel('Mean Variance of Wavelet Coefficients')\n",
        "plt.title('Mean Variance Across Participants at Each Decomposition Level')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# This plot can help identify if there's a consistent level where variance (and thus potential periodicity) is more pronounced."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import ttest_1samp\n",
        "\n",
        "# Assuming variance_df is your DataFrame of variances across participants with rows as participants and columns as decomposition levels\n",
        "\n",
        "def permutation_test(data, n_permutations=1000):\n",
        "    \"\"\"\n",
        "    Performs a permutation test on the data to assess the significance of the observed mean variance.\n",
        "    \n",
        "    Parameters:\n",
        "    - data: DataFrame containing the variances of wavelet coefficients across participants\n",
        "    - n_permutations: Number of permutations to perform\n",
        "    \n",
        "    Returns:\n",
        "    - p_values: P-values for each decomposition level, assessing the significance of the observed mean variance\n",
        "    \"\"\"\n",
        "    observed_means = data.mean()\n",
        "    p_values = pd.Series(index=observed_means.index, dtype=float)\n",
        "    \n",
        "    for level in data.columns:\n",
        "        observed_mean = observed_means[level]\n",
        "        distribution = []\n",
        "        \n",
        "        for _ in range(n_permutations):\n",
        "            # Shuffle the data within each level\n",
        "            shuffled = data[level].sample(frac=1, replace=False).reset_index(drop=True)\n",
        "            distribution.append(shuffled.mean())\n",
        "        \n",
        "        # Calculate the p-value as the proportion of shuffled means that are greater than or equal to the observed mean\n",
        "        p_value = (np.sum(distribution >= observed_mean) + 1) / (n_permutations + 1)\n",
        "        p_values[level] = p_value\n",
        "    \n",
        "    return p_values\n",
        "\n",
        "# Perform the permutation test\n",
        "p_values = permutation_test(variance_df, n_permutations=1000)\n",
        "\n",
        "# Display the p-values\n",
        "print(p_values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import pycwt as wavelet\n",
        "from pycwt.helpers import find\n",
        "\n",
        "# Because the time series might not be uniformly sampled, we create a uniform time series for the wavelet analysis.\n",
        "# Here, we assume 't' is approximately uniformly spaced for simplification. For more precise analysis, interpolation might be needed.\n",
        "\n",
        "# Aggregate data over time if necessary\n",
        "t = data['t'].unique()  # Assuming 't' values are uniformly spaced and consistent across participants\n",
        "mean_corr = data.groupby('t')['mean_corr'].mean().values  # Average mean_corr at each time point across all participants\n",
        "\n",
        "# Wavelet Transform\n",
        "dt = np.mean(np.diff(t))  # Calculate the mean time difference\n",
        "n = len(t)\n",
        "time = np.arange(0, n) * dt  # Create an array of time points\n",
        "p = np.polyfit(time, mean_corr, 1)  # Remove the linear trend for a better wavelet analysis\n",
        "mean_corr_detrend = mean_corr - np.polyval(p, time)\n",
        "\n",
        "# Define the wavelet to be used\n",
        "mother = wavelet.Morlet(6)\n",
        "s0 = 2 * dt  # Starting scale\n",
        "dj = 1 / 12  # Twelve sub-octaves per octaves\n",
        "J = 7 / dj  # Seven powers of two with dj sub-octaves\n",
        "alpha, _, _ = wavelet.ar1(mean_corr_detrend)  # Lag-1 autocorrelation for white noise\n",
        "\n",
        "# The following wavelet transform returns the wavelet power spectrum (WPS), scales, frequencies, coi, fft, fft frequencies, and significance levels.\n",
        "WPS, scales, freqs, coi, fft, fftfreqs, sig = wavelet.cwt(mean_corr_detrend, dt, dj, s0, J, mother)\n",
        "power = (np.abs(WPS)) ** 2\n",
        "significance_levels = wavelet.significance(1.0, dt, scales, 0, alpha, significance_level=0.95, wavelet=mother)\n",
        "\n",
        "# Visualizing the wavelet power spectrum\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.contourf(time, np.log2(freqs), np.log2(power), extend='both')\n",
        "plt.colorbar(label='Power (log2 scale)')\n",
        "plt.title('Wavelet Power Spectrum of mean_corr')\n",
        "plt.ylabel('Frequency (log2 scale)')\n",
        "plt.xlabel('Time')\n",
        "plt.tight_layout()\n",
        "\n",
        "# Overlay the cone of influence\n",
        "plt.contour(time, np.log2(freqs), sig, [-99, 1], colors='k', linewidths=2)\n",
        "plt.fill(np.concatenate([time, time[-1:] + dt, time[-1:] + dt, time[:1] - dt, time[:1] - dt]),\n",
        "         np.concatenate([np.log2(coi), [1e-9], np.log2(freqs)[-1:], np.log2(freqs)[-1:], [1e-9]]),\n",
        "         'k', alpha=0.3, hatch='x')\n",
        "\n",
        "# Indicate the significance levels with a contour\n",
        "plt.contour(time, np.log2(freqs), np.log2(power) - np.log2(significance_levels[:, None]), [-99, 0], colors='r', linewidths=2)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Calculate the global wavelet spectrum and its significance level\n",
        "global_ws = power.mean(axis=1)\n",
        "dof = n - scales  # Degrees of freedom\n",
        "global_significance = wavelet.significance(1.0, dt, scales, 1, alpha, significance_level=0.95, dof=dof, wavelet=mother)\n",
        "\n",
        "# Plot the global wavelet spectrum\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(np.log2(freqs), np.log2(global_ws), 'b-', label='Global Wavelet Spectrum')\n",
        "plt.plot(np.log2(freqs), np.log2(global_significance), 'r--', label='Significance Level')\n",
        "plt.xlabel('Frequency (log2 scale)')\n",
        "plt.ylabel('Power (log2 scale)')\n",
        "plt.title('Global Wavelet Spectrum')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Assessing the presence of significant periodicity\n",
        "significant_periods = freqs[np.where(global_ws >= global_significance[0])]\n",
        "p_value = 1 - 0.95 if significant_periods.size > 0 else 1  # Simplistic approach to demonstrate significance\n",
        "\n",
        "significant_periods, p_value\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}